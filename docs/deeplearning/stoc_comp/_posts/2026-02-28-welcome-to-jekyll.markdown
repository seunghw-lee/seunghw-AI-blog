---
layout: post
title:  "Why addewrner is not used widely? in stoc_comp"
date:   2026-02-28 20:02:17 +0900
category: 
    - deeplearning 
    - stoc_comp
---


Asking AI about this....

Back propagation을 understand 하는 수준까지 우선 공부해야겠다.....ㄷㄷㄷ

https://www.google.com/search?q=why+addernet+not+used+widely&oq=why+addernet+not+used+widely&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCjQzODU3ajBqMTWoAgCwAgA&sourceid=chrome&ie=UTF-8



AdderNet (Adder Neural Network) is not widely adopted despite being a promising, energy-efficient alternative to Convolutional Neural Networks (CNNs) primarily due to **significant training difficulties, specialized hardware requirements, and a, often, slight drop in accuracy**. While AdderNet replaces costly multiplication operations with cheaper addition operations, it introduces new challenges in optimization, stability, and implementation.

 

Here are the key reasons why AdderNet is not widely used:

- **Unstable and Complex Training:** Training AdderNets is significantly harder than standard CNNs. The loss curves are often unstable, leading to difficult convergence. This necessitates specialized techniques like adaptive learning rates and careful gradient management.
- **Hardware Bottlenecks and Lack of Optimization:** Modern deep learning hardware (GPUs) is highly optimized for matrix multiplications (MAC operations), not for the sum-of-absolute-differences method used in AdderNets. Without specialized hardware accelerators (like FPGAs with custom AdderNet kernels), the theoretical energy savings do not translate into faster real-world inference, sometimes resulting in higher latency.
- **Accuracy Degradation:** In many cases, AdderNets show a slight, albeit small, decrease in accuracy compared to well-trained CNN baselines (e.g., on CIFAR-100).
- **Massive Variance in Weights/Activations:** The values of weights and activations in AdderNets tend to have extremely large variances, which limits performance and makes them difficult to generalize across different tasks (like detection or segmentation).
- **Difficulties in Quantization:** While AdderNets aim to reduce reliance on floating-point multiplications, they are tricky to quantize for integer-only hardware. Their unique distribution of activations and weights leads to significant accuracy loss when standard quantization techniques are applied. ![Reddit](data:image/png;base64,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)Reddit +9

**Current Status:**
AdderNet remains a promising area of academic research for mobile/edge AI, particularly on FPGAs where it can achieve roughly 2.1x lower energy consumption. However, it has not replaced conventional CNNs in mainstream, large-scale deep learning applications.

 

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
